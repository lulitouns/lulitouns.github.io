<html>
	<head>
		<meta charset="UTF-8">
		<title> Présentation Maël Pegny </title>
           <link rel = "stylesheet" href="Csq.css"/> 
        <head>
          <meta charset="utf-8">
          <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,300;0,400;0,700;1,400&display=swap" rel="stylesheet">
      </head>
        <body>
            <div class="cv-container">
              <img id="en-tête" src="https://img.freepik.com/vecteurs-libre/visualisation-flux-donnees-flux-donnees-volumineuses-bleu-sous-forme-chaines-nombres-binaires-tordues-dans-tunnel-infini-representation-du-flux-code-information-analyse-cryptographique-transfert-blockchain-bitcoin_1217-2356.jpg?w=1380&t=st=1655367841~exp=1655368441~hmac=485d0d80dc667c94139960a10681a0d9fca0375f7799ebc5e391557144409848" alt="Banner Image"/>
              <img id="pied-de-page" src="https://img.freepik.com/vecteurs-libre/visualisation-flux-donnees-flux-donnees-volumineuses-bleu-sous-forme-chaines-nombres-binaires-tordues-dans-tunnel-infini-representation-du-flux-code-information-analyse-cryptographique-transfert-blockchain-bitcoin_1217-2356.jpg?w=1380&t=st=1655367841~exp=1655368441~hmac=485d0d80dc667c94139960a10681a0d9fca0375f7799ebc5e391557144409848" alt="Banner Image"/>
              <div class="left-column">
                <img class="portait" src="https://www.centerphilsci.pitt.edu/wp-content/uploads/2019/08/mael-pegny-312x234.jpg" />
                <div class="section">
                  <h2><span class="text-glace">INTELLIGENCE ARTIFICIELLE ET VIE PRIVÉE<br><br></span></h2>
                </div>
                <img class="thème" src="https://cdn2.nextinpact.com/images/bd/news/medium-169276.jpeg" />
              </div>
              <div class="right-column">
                <div class="content">
                  <div class="section">
                    <h2><span class="text-verdo">LES CONSÉQUENCES LÉGALES DU MACHIN LEARNING SUR LES DONNÉES PERSONNELLES<br><br></span></h2>
                    <p>
                      Le machine Learning lié au Big Data met fin au paradigme de protection par restriction de l’accès aux données. 
                      L’anonymisation de la donnée peut devenir totalement inutile face à la puissance d’un algorithme de machine learning. 
                      Concrètement, le chercheur démontre la capacité pour un algorithme d’établir des relations inférentielles entre données. 
                      Ces relations permettent à l’utilisateur de l’algorithme de « remonter » à une personne à partir de la seule relation entre des données anonymisées.
                      L’enjeu juridique posé par cette situation est le suivant : les relations inférentielles peuvent-elles être considérées comme des données personnelles en tant que telles ?
                      Il est important de prendre la mesure du problème, les outils de ML s’appuyant sur ces « relations/données » sont exploités à des fins commerciales, de suivi comportemental voire la définition pure et simple de qui nous sommes. 
                      Ces outils échappent à l’heure actuelle à tout contrôle, car sont exclus du champ d’application du RGPD à cause de la prétendue anonymisation des données exploitées.<br><br><br>
                      Car les modèles exploités par le machine learning sont nourris par des données privées et parfois sensibles, afin d’optimiser des résultats toujours plus pertinents, les risques liés à ces modèles sont en forte croissance. 
                      Si les programmes de machine learning font un usage important de données anonymisées, stockées de manière plus que sécurisée, il est particulièrement dur d’offrir les mêmes garanties sur les modèles exploités par le machine learning lui-même. 
                      Concrètement, avant d’être traitées, les données sont difficilement sécurisables, après traitement, elles sont anonymisées et sécurisées.
                      L’état de l’art permet de développer des méthodes d’attaques sur ces modèles. 
                      Une attaque de modèle de Machine Learning est un détournement de son usage, par le contournement de leurs fonctionnalités ou par la révélation de ses secrets. 
                      Ce dernier type d’attaque, appelé « privacy attack » ou attaque contre la confidentialité. 
                      C’est celui qui nous intéresse le plus dans le cadre de la protection des données à caractère personnel.<br><br><br>
                      Les attaques inférant l’appartenance sont les plus dures à réaliser et relèvent pour l’instant du domaine de la théorie. 
                      L’idée est simple : on choisit un individu et l’on cherche à savoir s’il est intégré à la base de données de l’entraînement. 
                      Par exemple, on infère qu’un individu est membre des patients dont les données ont été exploitées pour réaliser une étude médicale de grande échelle. 
                      Si l’assaillant réussit l’inférence (ce qui est peu probable), il accède de facto aux données de santé de l’individu étudié.
                      Les attaques par inversion de modèle sont les plus courantes, les plus fiables et les plus faciles à réaliser en raison du volume de données offert par le Big Data. 
                      Pour envisager cette attaque il faut comprendre à quel genre d’anonymisation procède le créateur de modèles de machine learning. 
                      Quand elle passe dans la black-box (cœur du programme de l’IA, difficilement envisageable et intelligible pour l’esprit humain), la machine procède à un hash ou à un chiffrement robuste de la donnée. 
                      Cela signifie par exemple qu’une photo précise devient un amas de pixels absolument indéchiffrable pour l’œil humain. 
                      C’est la technique utilisée dans le cadre des NFT par exemple. 
                      L’attaque intervient là, grâce au volume de données publiques (photos entre autres) du Big data, l’assaillant peut être en mesure de reconstituer une photo "hachée".
                      Cette possibilité offerte à l’attaquant de retrouver une donnée à caractère personnel à partir du seul outil lui-même impose que la destruction pure et simple de la donnée n’est pas suffisante pour la protéger...
                      Cette épineuse question des attaques de modèle pose un enjeu fondamental en matière de protection de la vie privée et des données à caractère personnel.
                      En effet, un des éléments caractéristiques des textes juridiques sur ce sujet est la durée de conservation des données personnelles. Le simple fait qu’une attaque par inversion de modèles, par extraction ou sur les données d’entraînement, soit capable de « remonter » des données à caractère personnel signifie par extension que la suppression pure et simple d’une donnée pas inutilisable. 
                      En outre, cette démonstration faite, il convient de s’interroger sur la pertinence de la définition actuelle de la « donnée personnelle ».<br><br><br>
                      Le droit des données s’intéresse et est centré sur les données avant un traitement (au sens technique, pas du RGPD) quelconque. 
                      Deux termes sont à comprendre : la source primaire et la donnée primaire/brute. 
                      Respectivement une information recueillie directement par l’auteur d’un document, les données non interprétées émanant d’une source primaire, ayant des caractéristiques liées à celle-ci et qui n'ont été soumises à aucun traitement ou toute autre manipulation.
                      Le RGPD admet pour autant à son considérant 26 qu’une donnée est personnelle si la technologie permet de retrouver une donnée personnelle à partir d’elle, même si elle n’est plus primaire donc, même si elle a été soumise à un traitement (le chiffrement par exemple).
                      Alors, pourquoi admettre une distinction profonde entre le programme et la donnée personnelle, si par invertibilité on peut déduire du programme des données personnelles ? 
                      Le Contrôleur européen à la Protection des données pose trois critères pour définir une donnée personnelle : son contenu, son intention et son résultat.
                      Cette interprétation juridique large, si elle était appliquée dans les faits, liée à la datafication ubiquitaire et au Machine Learning pose un risque : l’implosion juridique par saturation opérationnelle. 
                      Le volume des litiges deviendrait tel que le système juridique serait totalement inefficace.
                      Pour conclure, il apparaît que l’intelligence artificielle, et plus particulièrement le ML, en ce qu’ils bouleversent l’état de l’art, sont des menaces pour la conception que l’on se fait des données personnelles.
                      La fin du paradigme de protection des données par la restriction de leur accès nécessite une révision des concepts. 
                      Il semble important de se pencher sur la distinction donnée-programme-algorithme pour défendre efficacement les principes de la vie privée. 
                      La pertinence de la dichotomie entre anonyme et personnel, privé et public, sensible ou non apparaît totalement archaïque face à ces technologies avancées.<br><br><br>
                    </p>
                  </div>
                </div>
              </div>
            </div>
        </body>
  </head>
</html>